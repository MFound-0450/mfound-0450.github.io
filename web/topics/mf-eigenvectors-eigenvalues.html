
<h1>Eigenvalues and Eigenvectors</h1>

<p>For a square matrix <b>A</b>, \(\lambda\) is an eigenvalue if there exists a vector <b>x</b> such that:</p>

<p style="text-align: center;">\(\mathbf{A}\mathbf{x} = \lambda \mathbf{x},\)</p>

<p>where \(\mathbf{x} \neq \mathbf{0}\). The vector \(\mathbf{x}\) is called the <em>eigenvector</em> associated with the eigenvalue \(\lambda\).</p>

<h2>Characteristic Equation</h2>
<p>The equation above can be rewritten as:</p>

<p style="text-align: center;">\((\mathbf{A} - \lambda \mathbf{I})\mathbf{x} = \mathbf{0}.\)</p>

<p>This implies that:</p>

<p style="text-align: center;">\(\text{rank}(\mathbf{A} - \lambda \mathbf{I}) < n,\)</p>

<p>where \(n\) is the size of the matrix \(\mathbf{A}\). Consequently, the determinant must satisfy:</p>

<p style="text-align: center;">\(\text{det}(\mathbf{A} - \lambda \mathbf{I}) = 0.\)</p>

<h2>Collinearity and Codirection</h2>
<p>Two vectors are <em>codirected</em> if they point in the same direction. They are <em>collinear</em> if they point in the same or opposite direction.</p>


<h2>Non-uniqueness of Eigenvectors</h2>
<p>For a given eigenvalue \(\lambda\), the associated eigenvector is not unique. If \(\mathbf{A}\mathbf{x} = \lambda \mathbf{x}\), then for any scalar \(c\):</p>

<p style="text-align: center;">\(\mathbf{A}(c\mathbf{x}) = \lambda (c\mathbf{x}),\)</p>

<p>so \(c\mathbf{x}\) is also an eigenvector associated with the eigenvalue \(\lambda\).</p>

<h2>Finding Eigenvalues</h2>
<p>To obtain the eigenvalues of \(\mathbf{A}\), we solve the characteristic polynomial:</p>

<p style="text-align: center;">\(\text{det}(\mathbf{A} - \lambda \mathbf{I}) = 0.\)</p>

<p>The roots of the characteristic polynomial are the eigenvalues of \(\mathbf{A}\).</p>

<h2>Eigenspace and Eigenspectrum</h2>
<p>The set of all eigenvalues of \(\mathbf{A}\) is called the <em>eigenspectrum</em>. For a particular eigenvalue \(\lambda\), the set of all associated eigenvectors forms the <em>eigenspace</em> with respect to \(\lambda\).</p>

<h2>Properties of Eigenvalues and Eigenvectors</h2>
<ul>
    <li>\(\mathbf{A}\) and \(\mathbf{A}^\top\) possess the same eigenvalues but not necessarily the same eigenvectors.</li>
    <li>The eigenspace for an eigenvalue \(\lambda\) is the null space of \(\mathbf{A} - \lambda \mathbf{I}\).</li>
    <li>Similar matrices have the same eigenvalues.</li>
    <li>Symmetric positive definite matrices always have positive real eigenvalues.</li>
    <li>The eigenvectors of a matrix with \(n\) distinct eigenvalues are linearly independent.</li>
    <li>For a symmetric matrix, the eigenvalues are real, and there exist orthonormal eigenvectors associated with each eigenvalue.</li>
    <li>The determinant of a matrix \(\mathbf{A}\) is the product of its eigenvalues.</li>
    <li>The trace of a matrix \(\mathbf{A}\) is the sum of its eigenvalues.</li>
</ul>


<h3>References</h3>
<p>M. P. Deisenroth, A. A. Faisal, and C. S. Ong, ‘Mathematics for Machine Learning’. Cambridge University Press,  2020, pp. 19–26. Accessed: Jun. 26, 2024. [Online]. Available: https://mml-book.github.io/book/mml-book.pdf
</p>